{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Additional libraries added for project:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Data processing\nfrom sklearn.impute import KNNImputer\n\n#Model selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\n#ML algorthms\nfrom xgboost import XGBRegressor\n\n#Deep learning\nimport tensorflow as tf\n\n#other\n\nfrom IPython.display import Image, display\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-21T23:13:22.952792Z","iopub.execute_input":"2023-06-21T23:13:22.954085Z","iopub.status.idle":"2023-06-21T23:13:36.782743Z","shell.execute_reply.started":"2023-06-21T23:13:22.954029Z","shell.execute_reply":"2023-06-21T23:13:36.781260Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2>Project Scope</h2>\n\n   <ul>\n            <li>Using solar generation and associated temperature data to explore time series forecasting and make a short-term solar power forecasting model.</li>\n            <li>Examine and process outliers/missing data</li>\n            <li>Compare the performance of an XGBoost model vs a deep learning LSTM model to make day-ahead forecasts</li>\n            <li>See what impact including the temperature data has on the forecast</li>\n            <li>Experiment with feature engineering and test how differernt features effect the model's performance</li>\n            <li>Make a full pipeline of the best model.</li> \n            <li>A better way?</li>\n            <li>Report conclusions and comment on possible project extensions</li>\n   </ul>","metadata":{}},{"cell_type":"markdown","source":"<h2>Loading and exploring data</h2>\nThis data was taken from two solar plants in India over the course of 34 days in 15 minute intervals. The weather data is on the plant level (just measured from one sensor) and the generation data is gathered from individual inverters across the plant. More information can be found here: \n<a href=\"https://www.kaggle.com/datasets/anikannal/solar-power-generation-data\">Data Card</a>. Credit to \n<a href=\"https://www.kaggle.com/anikannal\">Ani Kannal</a> for uploading this dataset to Kaggle. ","metadata":{}},{"cell_type":"markdown","source":"### Generation data\nFor now we'll just look at the Plant 1 data.","metadata":{}},{"cell_type":"code","source":"df_plt1_gen = pd.read_csv('/kaggle/input/solar-power-generation-data/Plant_1_Generation_Data.csv')\n\ndf_plt1_gen.sample(10, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:36.785109Z","iopub.execute_input":"2023-06-21T23:13:36.786041Z","iopub.status.idle":"2023-06-21T23:13:38.190614Z","shell.execute_reply.started":"2023-06-21T23:13:36.786004Z","shell.execute_reply":"2023-06-21T23:13:38.181640Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_plt1_gen \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/solar-power-generation-data/Plant_1_Generation_Data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_plt1_gen\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/solar-power-generation-data/Plant_1_Generation_Data.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/solar-power-generation-data/Plant_1_Generation_Data.csv'","output_type":"error"}]},{"cell_type":"markdown","source":"PLANT_ID is the same throughout, so it can be safely removed. The generation is 0 for nighttime, which of course makes sense for solar power. Also, let's rename to inverters/inverter column to make them easier to track","metadata":{}},{"cell_type":"code","source":"#Since we're focusing on plant 1 for now we'll shorten the name\ndf_gen = df_plt1_gen.drop('PLANT_ID', axis=1)\n\ndf_gen['INVERTER'] = df_gen.SOURCE_KEY.map({df_gen.SOURCE_KEY.unique()[i-1]: f'INVERTER_{i}' for i in range(1, len(df_gen.SOURCE_KEY.unique()) +1)})\ndf_gen = df_gen.drop('SOURCE_KEY', axis=1)\ninverters = df_gen.INVERTER.unique()\ninverters","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.191931Z","iopub.status.idle":"2023-06-21T23:13:38.192440Z","shell.execute_reply.started":"2023-06-21T23:13:38.192196Z","shell.execute_reply":"2023-06-21T23:13:38.192219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_gen.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.194340Z","iopub.status.idle":"2023-06-21T23:13:38.195083Z","shell.execute_reply.started":"2023-06-21T23:13:38.194869Z","shell.execute_reply":"2023-06-21T23:13:38.194892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_gen.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.196353Z","iopub.status.idle":"2023-06-21T23:13:38.197039Z","shell.execute_reply.started":"2023-06-21T23:13:38.196835Z","shell.execute_reply":"2023-06-21T23:13:38.196857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_gen.groupby('INVERTER').count().DATE_TIME.sort_values()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.198283Z","iopub.status.idle":"2023-06-21T23:13:38.198698Z","shell.execute_reply.started":"2023-06-21T23:13:38.198505Z","shell.execute_reply":"2023-06-21T23:13:38.198524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dtypes make sense, but we'll need to make the DATE_TIME a datetime object for easier analysis. No nulls detected, but it looks like there could be missing data either at the plant or inverter level. At the plant level we would expect 3264 unique timestamps (34 days * 24 hours * 4 (15 minute intervals in hour)), but there are only 3158. Across all inverters there would be 71808 rows (3264 * 22 (inverters)), but we only have 68778. So overall, around 3000 or ~4% of the total expected are missing. There is also some variance per inverter for what datetimes are missing.","metadata":{}},{"cell_type":"code","source":"df_gen['DATE_TIME'] = pd.to_datetime(df_gen.DATE_TIME, format='%d-%m-%Y %H:%M')\ndf_gen.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.200716Z","iopub.status.idle":"2023-06-21T23:13:38.201159Z","shell.execute_reply.started":"2023-06-21T23:13:38.200934Z","shell.execute_reply":"2023-06-21T23:13:38.200961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weather sensor data","metadata":{}},{"cell_type":"code","source":"df_plt1_weather = pd.read_csv('/kaggle/input/solar-power-generation-data/Plant_1_Weather_Sensor_Data.csv')\n\ndf_plt1_weather.sample(10, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.202463Z","iopub.status.idle":"2023-06-21T23:13:38.202869Z","shell.execute_reply.started":"2023-06-21T23:13:38.202676Z","shell.execute_reply":"2023-06-21T23:13:38.202696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_plt1_weather.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.204318Z","iopub.status.idle":"2023-06-21T23:13:38.204718Z","shell.execute_reply.started":"2023-06-21T23:13:38.204520Z","shell.execute_reply":"2023-06-21T23:13:38.204539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_plt1_weather.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.205723Z","iopub.status.idle":"2023-06-21T23:13:38.206086Z","shell.execute_reply.started":"2023-06-21T23:13:38.205905Z","shell.execute_reply":"2023-06-21T23:13:38.205922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can remove the PLANT_ID and SOURCE_KEY since they are the same throughout. Let's again make DATE_TIME a datetime object. Based on the DATE_TIME count we also have some missing weather data. ","metadata":{}},{"cell_type":"code","source":"df_weather = df_plt1_weather.drop(['PLANT_ID', 'SOURCE_KEY'], axis=1)\ndf_weather['DATE_TIME'] = pd.to_datetime(df_weather.DATE_TIME, format='%Y-%m-%d %H:%M:%S') ","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.207521Z","iopub.status.idle":"2023-06-21T23:13:38.207914Z","shell.execute_reply.started":"2023-06-21T23:13:38.207722Z","shell.execute_reply":"2023-06-21T23:13:38.207742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling out missing datetimes\nIn order to fill in the missing datetime stamps for our generation and weather sensor dataframes let's make a datetime object with the full range of expected datetimes.","metadata":{}},{"cell_type":"code","source":"datetimes_full = pd.Series(pd.date_range(df_gen.DATE_TIME.min(), df_gen.DATE_TIME.max(), freq='15min'), name='DATE_TIME')\ndatetimes_full","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.209451Z","iopub.status.idle":"2023-06-21T23:13:38.209820Z","shell.execute_reply.started":"2023-06-21T23:13:38.209636Z","shell.execute_reply":"2023-06-21T23:13:38.209652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to merge this will our dataframes. Starting with the generation data. Since different inverters have some different timestamps missing we will need to do this one inverter at a time and then concatenate them back together.","metadata":{}},{"cell_type":"code","source":"dfs_gen = []\nfor i in inverters:\n    df_inverter = df_gen[df_gen.INVERTER == i]\n    df_inverter = df_inverter.merge(datetimes_full, on='DATE_TIME', how='right')\n    df_inverter['INVERTER'] = i\n    dfs_gen.append(df_inverter)\ndf_gen = pd.concat(dfs_gen)\ndf_gen.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.210778Z","iopub.status.idle":"2023-06-21T23:13:38.211132Z","shell.execute_reply.started":"2023-06-21T23:13:38.210955Z","shell.execute_reply":"2023-06-21T23:13:38.210972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can add the weather data to this as well to make our full dataframe.","metadata":{}},{"cell_type":"code","source":"df = df_gen.merge(df_weather, on='DATE_TIME', how='left')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.212851Z","iopub.status.idle":"2023-06-21T23:13:38.213219Z","shell.execute_reply.started":"2023-06-21T23:13:38.213038Z","shell.execute_reply":"2023-06-21T23:13:38.213054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\nTo start our more in-depth analysis, we'll can make some basic datetime features. If we were modeling throughout the year(s), month and year could be interesting to account for seasonal variation and long-term trends, but since our data only covers 34 days, we will omit them. We can use dayofyear to capture any longer trends that might be present.","metadata":{}},{"cell_type":"code","source":"df['HOUR'] = df.DATE_TIME.dt.hour\ndf['DAY'] = df.DATE_TIME.dt.dayofyear\ndf['DAY_WEEK'] = df.DATE_TIME.dt.dayofweek\ndf['MINUTES_15'] = df.DATE_TIME.dt.time\n\n#This maps the 15 minute intervals over the course of the day to ints 1-96. \ndf['MINUTES_15'] = df.MINUTES_15.map({df.MINUTES_15.unique()[i-1]:i for i in range(1, 97)})\n#Change day of year to day of data\ndf['DAY'] = df.DAY.map({df.DAY.unique()[i-1]:i for i in range(1, 35)})","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.214366Z","iopub.status.idle":"2023-06-21T23:13:38.214747Z","shell.execute_reply.started":"2023-06-21T23:13:38.214562Z","shell.execute_reply":"2023-06-21T23:13:38.214579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check on the linear correlation of the features.","metadata":{}},{"cell_type":"code","source":"corr = df.corr(numeric_only=True)\ncorr","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.215701Z","iopub.status.idle":"2023-06-21T23:13:38.216055Z","shell.execute_reply.started":"2023-06-21T23:13:38.215876Z","shell.execute_reply":"2023-06-21T23:13:38.215892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And let's make it a heatmap.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(corr);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.218418Z","iopub.status.idle":"2023-06-21T23:13:38.223109Z","shell.execute_reply.started":"2023-06-21T23:13:38.222717Z","shell.execute_reply":"2023-06-21T23:13:38.222762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not surprisingly DC and AC power are highly correlated. This is good! It probably means the inverters in the plant are working correctly to convert the DC to AC. We will ultimately make AC_POWER our target. There is a strong correlation between HOUR and DAILY_YIELD, which makes sense as the daily yield increases throughout the day. For the weather sensor data there is not surprisingly a strong correlation between IRRADIATION and AC_POWER. Also between MODULE_TEMPERATURE and AC_POWER. Now we will look at a pairplot to see another representation of these relationships and look for any non-linear correlations.","metadata":{}},{"cell_type":"code","source":"# g = sns.PairGrid(df.sample(10000, random_state=1), diag_sharey=False, hue='INVERTER')\n# g.map_upper(sns.scatterplot, s=15)\n# g.map_lower(sns.kdeplot)\n# g.map_diag(sns.histplot)\n# #This takes quite a while to run, so saving this figure for future use.\n# plt.savefig(\"gen_pairgrid.png\", dpi=400)\n# display(Image(filename='gen_pairgrid.png'))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.225044Z","iopub.status.idle":"2023-06-21T23:13:38.225534Z","shell.execute_reply.started":"2023-06-21T23:13:38.225315Z","shell.execute_reply":"2023-06-21T23:13:38.225335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is A LOT of information to take in here! The zeros dominate the distribution for AC_POWER and DAILY_YIELD due to the nighttime. As we just saw perviously, there is further evidence of the missing data based on the DAY/MINUTES_15 pair. The inverters start to seperate out based on TOTAL_YIELD and DAILY_YIELD. This suggests the they are operating a different capacities; probably due to any number of factors: age, location within the plant, need for maintaince, etc. Exploring this more is outside the scope of this project, but could be a good starting point for another project. Finally, the connection between HOUR and AC_POWER is interesting and also makes sense with the peaks in the middle of the day. Let's explore this more next and also start to look for outliers.","metadata":{}},{"cell_type":"markdown","source":"## Outliers\nFirst a quick check for any negative power numbers.","metadata":{}},{"cell_type":"code","source":"df[['AC_POWER', 'DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD']].any() < 0","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.227251Z","iopub.status.idle":"2023-06-21T23:13:38.227724Z","shell.execute_reply.started":"2023-06-21T23:13:38.227505Z","shell.execute_reply":"2023-06-21T23:13:38.227527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They should only be positive and luckily that appears to be the case.","metadata":{}},{"cell_type":"markdown","source":"### Target outliers\nLet's now explore our target, AC_POWER. We can define an outlier as \\< 1th percentile or \\> 99th percentile. To do this I will make four quantile features below: 2 of them in the outlier range and 2 to establish 1 standard deviation from the mean. Then we can plot the results. ","metadata":{}},{"cell_type":"code","source":"#Making a new dataframe for EDA since we won't necessarily want these features for our model.\ndf_eda = df.copy()\ndf_eda = df_eda.merge(df_eda.groupby('MINUTES_15').quantile(0.01, numeric_only=True).AC_POWER.rename('OUTLIERS_LOW_AC_POWER'), on='MINUTES_15', how='left')\ndf_eda = df_eda.merge(df_eda.groupby('MINUTES_15').quantile(0.99, numeric_only=True).AC_POWER.rename('OUTLIERS_HIGH_AC_POWER'), on='MINUTES_15', how='left')\ndf_eda = df_eda.merge(df_eda.groupby('MINUTES_15').mean(numeric_only=True).AC_POWER.rename('MEAN'), on='MINUTES_15', how='left')\ndf_eda = df_eda.merge(df_eda.groupby('MINUTES_15').std(numeric_only=True).AC_POWER.rename('STD'), on='MINUTES_15', how='left')\ndf_eda['STD_1'] = df_eda.query('AC_POWER < (MEAN + STD) and AC_POWER > (MEAN - STD)').AC_POWER","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.229295Z","iopub.status.idle":"2023-06-21T23:13:38.229834Z","shell.execute_reply.started":"2023-06-21T23:13:38.229559Z","shell.execute_reply":"2023-06-21T23:13:38.229586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since we have nulls in our data now we will exclude those with df[~df.isna()]\nfig, ax = plt.subplots()\nsns.color_palette(\"Paired\")\nsns.scatterplot(data=df_eda[~df_eda.isna()], y='AC_POWER', x='MINUTES_15', hue='INVERTER', palette='gray', alpha=0.05, legend=False)\nsns.scatterplot(data=df_eda[~df_eda.isna()], y='STD_1', x='MINUTES_15', hue='INVERTER', palette='gray', alpha =0.5, legend=False)\nsns.scatterplot(data=df_eda[~df_eda.isna()].query('AC_POWER > OUTLIERS_HIGH_AC_POWER'), y='AC_POWER', x='MINUTES_15', hue='INVERTER', size='AC_POWER')\nsns.scatterplot(data=df_eda[~df_eda.isna()].query('AC_POWER < OUTLIERS_LOW_AC_POWER'), y='AC_POWER', x='MINUTES_15', hue='INVERTER', size= -df_eda[~df_eda.isna()].AC_POWER) \n\nplt.title('AC Power per 15 minutes')\nplt.ylabel('AC Power (kW)')\nplt.xlabel('Hour of Day')\nax.set_xticks([i for i in range(1, 97, 4)])\nax.set_xticklabels([i for i in range(24)])\nax.legend(['Full data', '+/- 1\\u03C3', 'Outliers']);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.231475Z","iopub.status.idle":"2023-06-21T23:13:38.231951Z","shell.execute_reply.started":"2023-06-21T23:13:38.231747Z","shell.execute_reply":"2023-06-21T23:13:38.231767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, it looks like there are several instances where the power generated during the middle of the day was 0. This could either indicate bad data, malfunctioning invererters, some kind of planned maintence, really cloudy days, really sunny days where the plant is overheating/at capacity or any number of other things. Without having a deeper domain knowledge it is hard to know for sure, but let's see if we can find any pattern to these mid-day outliers.\n\nFirst, we'll check to see if any inverters in particular are responsible.","metadata":{}},{"cell_type":"code","source":"outliers_low = df_eda[~df_eda.isna()].query('AC_POWER < OUTLIERS_LOW_AC_POWER')\noutliers_high = df_eda[~df_eda.isna()].query('AC_POWER > OUTLIERS_HIGH_AC_POWER')\n\noutliers_source_low = outliers_low.groupby('INVERTER').count().DATE_TIME.reset_index().sort_values('DATE_TIME', ascending=False)\noutliers_source_low_zero = outliers_low[outliers_low.AC_POWER == 0].groupby('INVERTER').count().DATE_TIME.reset_index().sort_values('DATE_TIME', ascending=False)\nfig, ax = plt.subplots()\nsns.barplot(data = outliers_source_low, y='INVERTER', x='DATE_TIME', alpha = 0.5, )\nsns.barplot(data = outliers_source_low_zero, y='INVERTER', x='DATE_TIME')\nplt.title('Outlier Counts by Inverter (shading for 0.0 AC Power)')\nplt.ylabel('Inverter')\nplt.xlabel('Count');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.233500Z","iopub.status.idle":"2023-06-21T23:13:38.233900Z","shell.execute_reply.started":"2023-06-21T23:13:38.233708Z","shell.execute_reply":"2023-06-21T23:13:38.233726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The spread of outlier counts, including the 0.0 AC_POWER measurements are concentrated in inverters 11 and 1. It could be that these were offline for an extended period due to maintance or malfunction. Below we'll look at the spread of the outliers over the course of the full 34 days to see if there is any regularity.","metadata":{}},{"cell_type":"code","source":"outliers_zero_count = outliers_low[outliers_low.AC_POWER == 0].groupby(['INVERTER','DAY']).count().rename(columns={'AC_POWER':'COUNTS'}).COUNTS.reset_index()\noutliers_zero_count\nfig, ax = plt.subplots(figsize=(8,5))\nsns.scatterplot(data=outliers_zero_count, x='DAY', y='COUNTS', hue='INVERTER', size='COUNTS', legend='brief')\nax.legend(bbox_to_anchor=(1, 1.05))\nax.set_xticks([i for i in range(1,35)]);\nax.set_xticklabels([]);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.235605Z","iopub.status.idle":"2023-06-21T23:13:38.236040Z","shell.execute_reply.started":"2023-06-21T23:13:38.235851Z","shell.execute_reply":"2023-06-21T23:13:38.235870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Several occur on the same day. There could have been maintenance that day or another issue. Now let's check for weekly patterns that could suggest planned maintenance.","metadata":{}},{"cell_type":"code","source":"outliers_zero_count = outliers_low[outliers_low.AC_POWER == 0].groupby(['INVERTER','DAY_WEEK']).count().rename(columns={'AC_POWER':'COUNTS'}).COUNTS.reset_index()\noutliers_zero_count\nfig, ax = plt.subplots(figsize=(8,2.5))\nsns.scatterplot(data=outliers_zero_count, x='DAY_WEEK', y='COUNTS', hue='INVERTER', size='COUNTS', legend=False);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.237203Z","iopub.status.idle":"2023-06-21T23:13:38.237624Z","shell.execute_reply.started":"2023-06-21T23:13:38.237382Z","shell.execute_reply":"2023-06-21T23:13:38.237423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, several of the zeros occur on Sunday. This could still be coincidence or it might suggest some scheduled maintance. Since Sunday is likely a day with less demand on the grid, it would make sense to choose that as a day for maintance or some planned outage. Finally, lets see how these outliers relate to the irradiation. distribution, since the two should be highly correlated. Let's circle back and check on the non-zero outliers as well.","metadata":{}},{"cell_type":"code","source":"outliers_low_count = outliers_low.groupby(['INVERTER','DAY_WEEK']).count().rename(columns={'AC_POWER':'COUNTS'}).COUNTS.reset_index()\nfig, ax = plt.subplots(figsize=(8,2.5))\nsns.scatterplot(data=outliers_low_count, x='DAY_WEEK', y='COUNTS', hue='INVERTER', size='COUNTS', legend=False);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.239648Z","iopub.status.idle":"2023-06-21T23:13:38.240031Z","shell.execute_reply.started":"2023-06-21T23:13:38.239841Z","shell.execute_reply":"2023-06-21T23:13:38.239865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a greater number of overall outliers on Monday. We might guess that after Sunday maintance there is a delay getting everything back online that goes into Monday. I think there is enough evidence now to choose to keep these outliers in our data as it might help the model account for some of this. That being said the number is low enough that it likely won't have too much of an impact.","metadata":{}},{"cell_type":"markdown","source":"### Other Outliers\nLet's take a quick look at outliers in the other features, before moving to missing data. Since we'll be exploring several different features we can make a function to help streamline the process.","metadata":{}},{"cell_type":"code","source":"def outliers(df, feature, high_per, low_per, source_key=True):\n    df = df.copy()\n    df = df.merge(df.groupby('MINUTES_15').quantile(low_per, numeric_only=True)[feature].rename(f'OUTLIERS_LOW_{feature}'), on='MINUTES_15', how='left')\n    df = df.merge(df.groupby('MINUTES_15').quantile(high_per, numeric_only=True)[feature].rename(f'OUTLIERS_HIGH_{feature}'), on='MINUTES_15', how='left')\n    df = df.merge(df.groupby('MINUTES_15').mean(numeric_only=True)[feature].rename(f'MEAN_{feature}'), on='MINUTES_15', how='left')\n    df = df.merge(df.groupby('MINUTES_15').std(numeric_only=True)[feature].rename(f'STD_{feature}'), on='MINUTES_15', how='left')\n    df[f'STD_1_{feature}'] = df.query(f'{feature} < (MEAN_{feature} + STD_{feature}) and {feature} > (MEAN_{feature} - STD_{feature})')[feature]\n    \n    fig, ax = plt.subplots()\n    sns.color_palette(\"Paired\")\n    if source_key == True:\n        sns.scatterplot(data=df, y=feature, x='MINUTES_15', hue='INVERTER', palette='gray', alpha=0.05, legend=False)\n        sns.scatterplot(data=df, y=f'STD_1_{feature}', x='MINUTES_15', hue='INVERTER', palette='gray', alpha =0.5, legend=False)\n        sns.scatterplot(data=df.query(f'{feature} > OUTLIERS_HIGH_{feature}'), y=feature, x='MINUTES_15', hue='INVERTER', size=feature)\n        sns.scatterplot(data=df.query(f'{feature} < OUTLIERS_LOW_{feature}'), y=feature, x='MINUTES_15', hue='INVERTER', size= -df[feature]) \n    else:\n        sns.scatterplot(data=df, y=feature, x='MINUTES_15', color='gray', alpha=0.05, legend=False)\n        sns.scatterplot(data=df, y=f'STD_1_{feature}', x='MINUTES_15', color='gray', alpha =0.5, legend=False)\n        sns.scatterplot(data=df.query(f'{feature} > OUTLIERS_HIGH_{feature}'), color='blue', y=feature, x='MINUTES_15', size=feature)\n        sns.scatterplot(data=df.query(f'{feature} < OUTLIERS_LOW_{feature}'), color='red', y=feature, x='MINUTES_15', size= -df[feature])\n        \n    plt.title(f'{feature} per 15 minutes')\n    plt.ylabel(f'{feature}')\n    plt.xlabel('Hour of Day')\n    ax.set_xticks([i for i in range(1, 97, 4)])\n    ax.set_xticklabels([i for i in range(24)])\n    ax.legend(['Full data', '+/- 1\\u03C3', 'Outliers']);\n    return ax, df","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.241780Z","iopub.status.idle":"2023-06-21T23:13:38.242174Z","shell.execute_reply.started":"2023-06-21T23:13:38.241978Z","shell.execute_reply":"2023-06-21T23:13:38.241995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax_daily_yield, df_daily_yield = outliers(df_eda[~df_eda.isna()], 'DAILY_YIELD', 0.99, 0.01);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.242975Z","iopub.status.idle":"2023-06-21T23:13:38.243345Z","shell.execute_reply.started":"2023-06-21T23:13:38.243161Z","shell.execute_reply":"2023-06-21T23:13:38.243179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some unusual things happening here. Mainly the 0s after 6pm. The straight line down at the end of the day and the other line at the start. These seem like they could be incorrect data, or maybe some kind of correction of previous data. Whatever their exact cause, it is probably best to correct these to make the data less disconnected and ultimately help our model. To fix these we'll make every value during the night equal to the maximum daily yield and set all the daily yields at the beginning of the day to 0.","metadata":{}},{"cell_type":"code","source":"#Had a go through a few trys to settle on MINUTES_15 = 75 as a good starting point for night.\nnighttime = [i for i in range(75,97)]\n#Making these changes to the original df\ndf['DAILY_YIELD'] = df['DAILY_YIELD'].mask(df.HOUR.isin([0]), 0)\n#We need a maximun daily yield for each inverter seperately\ndaily_yield = df.copy()\ndaily_yield = df.merge(pd.DataFrame({'DAILY_YIELD_DAY_MAX' : df.groupby([ 'INVERTER', 'DAY']).max()['DAILY_YIELD']}).reset_index(), on=['INVERTER', 'DAY'], how='right')\ndf['DAILY_YIELD'] = df['DAILY_YIELD'].mask(df.MINUTES_15.isin(nighttime), daily_yield['DAILY_YIELD_DAY_MAX'])\n\n#Let's also set the nighttime generation for AC_POWER and DC_POWER to 0.\ndf['AC_POWER'] = df['AC_POWER'].mask(df.MINUTES_15.isin(nighttime), 0)\ndf['DC_POWER'] = df['DC_POWER'].mask(df.MINUTES_15.isin(nighttime), 0)\n\nax_daily_yield, df_daily_yield = outliers(df[~df.isna()], 'DAILY_YIELD', 0.99, 0.01);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.244757Z","iopub.status.idle":"2023-06-21T23:13:38.245149Z","shell.execute_reply.started":"2023-06-21T23:13:38.244952Z","shell.execute_reply":"2023-06-21T23:13:38.244969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is better now. It remains to be seen how much of an inpact daily yield has on our model, but this should help at least a little.","metadata":{}},{"cell_type":"markdown","source":"## Imputing missing values\nNow we are ready to impute the missing values as a final step before beginning our modeling. First, let's find a section of missing data to use as a test case of how effective our imputer is.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=df[~df.isna()], x='MINUTES_15', y='DAY', hue='INVERTER', legend=False)\nplt.title('Data points');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.246830Z","iopub.status.idle":"2023-06-21T23:13:38.247580Z","shell.execute_reply.started":"2023-06-21T23:13:38.247344Z","shell.execute_reply":"2023-06-21T23:13:38.247364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a fair bit of missing day time data on day 6, so we can use this day to test our imputer with a before and after plot","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=df[~df.isna()][df.DAY == 6], x='MINUTES_15', y='AC_POWER', hue='INVERTER', legend=False);\nplt.title('Day 6 AC Power before imputing');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.248821Z","iopub.status.idle":"2023-06-21T23:13:38.249177Z","shell.execute_reply.started":"2023-06-21T23:13:38.249001Z","shell.execute_reply":"2023-06-21T23:13:38.249018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's begin imputing by setting all nighttime AC power generation to 0.","metadata":{}},{"cell_type":"code","source":"df['AC_POWER'] = df['AC_POWER'].mask(df.HOUR.isin([0]), 0)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.250251Z","iopub.status.idle":"2023-06-21T23:13:38.250666Z","shell.execute_reply.started":"2023-06-21T23:13:38.250463Z","shell.execute_reply":"2023-06-21T23:13:38.250482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can impute the other missing values. Let's try a couple of different methods and see what is more effective. We'll pick sklearn KNNImputer and also, based on this <a href=\"https://drnesr.medium.com/filling-gaps-of-a-time-series-using-python-d4bfddd8c460\">article</a>, the built-in pandas method interpolate, with the time method specially. For best results we will apply our imputer to each inverter individually and then concatenate the results.","metadata":{}},{"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=10)\ninverter_dfs = []\ninverter_df = None\nfor i in inverters:\n    inverter_df = df[df.INVERTER == i]\n    inverter_df.pop('INVERTER')\n    inverter_df.index = inverter_df['DATE_TIME']\n    datetime = inverter_df.pop('DATE_TIME')\n    inverter_df = pd.DataFrame(imputer.fit_transform(inverter_df), columns=inverter_df.columns, index=datetime)\n    inverter_df['INVERTER'] = i\n    inverter_df = inverter_df.reset_index()\n    inverter_dfs.append(inverter_df)\ndf_knn = pd.concat(inverter_dfs)\n\nsns.scatterplot(data=df_knn[df_knn.DAY == 6], x='MINUTES_15', y='AC_POWER', hue='INVERTER', legend=False);\nplt.title('Day 6 AC Power Imputed with KNN');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.251850Z","iopub.status.idle":"2023-06-21T23:13:38.252239Z","shell.execute_reply.started":"2023-06-21T23:13:38.252034Z","shell.execute_reply":"2023-06-21T23:13:38.252051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inverter_dfs = []\ninverter_df = None\nfor i in inverters:\n    inverter_df = df[df.INVERTER == i]\n    inverter_df.pop('INVERTER')\n    inverter_df.index = inverter_df['DATE_TIME']\n    datetime = inverter_df.pop('DATE_TIME')\n    inverter_df = inverter_df.interpolate(method='time')\n    inverter_df['INVERTER'] = i\n    inverter_df = inverter_df.reset_index()\n    inverter_dfs.append(inverter_df)\ndf_interpolate = pd.concat(inverter_dfs)\nsns.scatterplot(data=df_interpolate[df_interpolate.DAY == 6], x='MINUTES_15', y='AC_POWER', hue='INVERTER', legend=False);\n\nplt.title('Day 6 AC Power Imputed with interpolate(time method)');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.253240Z","iopub.status.idle":"2023-06-21T23:13:38.253657Z","shell.execute_reply.started":"2023-06-21T23:13:38.253462Z","shell.execute_reply":"2023-06-21T23:13:38.253482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both imputation methods seem to work pretty well, however, the KNNImputer appears to capture a bit more of the nuance of the data though, so we will choose that. Let's also make an imputer function.","metadata":{}},{"cell_type":"code","source":"def imputer(df, imputer=KNNImputer(n_neighbors=10)):\n    imputer = imputer\n    inverter_dfs = []\n    inverter_df = None\n    for i in inverters:\n        inverter_df = df[df.INVERTER == i]\n        inverter_df.pop('INVERTER')\n        inverter_df.index = inverter_df['DATE_TIME']\n        datetime = inverter_df.pop('DATE_TIME')\n        inverter_df = pd.DataFrame(imputer.fit_transform(inverter_df), columns=inverter_df.columns, index=datetime)\n        inverter_df['INVERTER'] = i\n        inverter_df = inverter_df.reset_index()\n        inverter_dfs.append(inverter_df)\n    df = pd.concat(inverter_dfs)\n    return df\n\ndf = imputer(df)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.256040Z","iopub.status.idle":"2023-06-21T23:13:38.256784Z","shell.execute_reply.started":"2023-06-21T23:13:38.256449Z","shell.execute_reply":"2023-06-21T23:13:38.256482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ## How to handle inverters? / Weather impact\nSo far we have had all of the inverters in one column of our dataframe with all of the their individual generation data in additional rows. For modeling though we should aggregate our dataframe to one datatime and make that our index. This leaves the question of what to do with the inverters. We could: aggregate accross them, greatly simplifying our dataframe, but possibly losing helpful information; or pivot them as new features to our dataframe making it much more complex; model each inverter seperately and make our model the sum of their predicitons. Also, as per our project scope, let's use this chance to see how the weather data impacts the models performance. We'll setup for each scenerio below -->","metadata":{}},{"cell_type":"markdown","source":"## Target","metadata":{}},{"cell_type":"markdown","source":"Before establishing a baseline we need to create our target. As mentioned previously our target will be the sum of the AC power generation between all the inverters for a given 15 minute interval forecasted ahead to the next day. So let's make the target and examine it a bit more closely before establishing a baseline.","metadata":{}},{"cell_type":"code","source":"day = 96\n#shifting to day ahead and droping nulls cause by the shift\ntarget = df.groupby('DATE_TIME').sum(numeric_only=True).AC_POWER.shift(-day)\ntarget = target.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.258183Z","iopub.status.idle":"2023-06-21T23:13:38.258768Z","shell.execute_reply.started":"2023-06-21T23:13:38.258477Z","shell.execute_reply":"2023-06-21T23:13:38.258504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at our target over one week.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\nsns.lineplot(target[target.index > '2020-06-10'])\nplt.title('One Week of Power Generation (kW)')\nplt.xlabel('Date')\nplt.ylabel('kW');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.260318Z","iopub.status.idle":"2023-06-21T23:13:38.260906Z","shell.execute_reply.started":"2023-06-21T23:13:38.260618Z","shell.execute_reply":"2023-06-21T23:13:38.260645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is what we will be modeling. There are a lot valleys and sharp peaks. Modeling the inverters seperately or including all the inverter generation data in the model could help address this.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\nsns.lineplot(df[(df.INVERTER == inverters[5]) & (df.DATE_TIME > '2020-06-10')].AC_POWER)\nplt.title('Inverter 5: One Week of Power Generation (kW)')\nplt.xlabel('Date')\nplt.ylabel('kW');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.262754Z","iopub.status.idle":"2023-06-21T23:13:38.263322Z","shell.execute_reply.started":"2023-06-21T23:13:38.263030Z","shell.execute_reply":"2023-06-21T23:13:38.263055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, looking at just one inverter actually doesn't seem to smooth out the data any, so the difference between including the inverter data seperately or just aggregating might be neglible. Either way, we'll explore both to compare the results. Let's look at the houlry results.","metadata":{}},{"cell_type":"code","source":"target_hour = target.resample('H').sum()\n\nfig, ax = plt.subplots(figsize=(15,5))\nsns.lineplot(target_hour[target_hour.index > '2020-06-10'])\nplt.title('One Week of Hourly Power Generation (kW)')\nplt.xlabel('Date')\nplt.ylabel('kW');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.264957Z","iopub.status.idle":"2023-06-21T23:13:38.269715Z","shell.execute_reply.started":"2023-06-21T23:13:38.269428Z","shell.execute_reply":"2023-06-21T23:13:38.269458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is much smoother and might be easier for the model to handle. Although it will also have less data to draw from. I will try both the 15min and hourly data to compare how they model, even though are ultimate goal is the 15 min AC power generation.","metadata":{}},{"cell_type":"code","source":"target_day = target.resample('D').sum()\nfig, ax = plt.subplots(figsize=(15,5))\nsns.lineplot(target_day)\nplt.title('Daily Power Generation (kW) over whole dataset')\nplt.xlabel('Date')\nplt.ylabel('kW');","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.270757Z","iopub.status.idle":"2023-06-21T23:13:38.271120Z","shell.execute_reply.started":"2023-06-21T23:13:38.270939Z","shell.execute_reply":"2023-06-21T23:13:38.270956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like there might be some longer trends present in the generation, possilby due to the weather patterns. We will try to include some features that could capture this in our model.","metadata":{}},{"cell_type":"markdown","source":"## Time series decomposition","metadata":{}},{"cell_type":"code","source":"result=seasonal_decompose(target, model='additive', period=96*7);\nresult.plot();","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.272012Z","iopub.status.idle":"2023-06-21T23:13:38.272356Z","shell.execute_reply.started":"2023-06-21T23:13:38.272178Z","shell.execute_reply":"2023-06-21T23:13:38.272194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decomposing our target captures some of the trends a bit better. Of course, the daily pattern is again clear. The residual plot does show quite a bit of noise, which will be challenging for our model to handle. Maybe some of the exogenous features can help with this. ","metadata":{}},{"cell_type":"markdown","source":"## Establishing Baseline\nWe are now ready to establish a baseline. Our baseline will be that the generation was the same as the day before at the same 15 minute interval. We will use root mean squared error (RMSE) scoring throughout and set aside the last 4 days of data to test between models. This will give us 3 days to work with since we are forecasting a day ahead. RMSE is used to give our model an extra penalty for large errors (since the errors are squared in the calculation). The smaller the score the better. ","metadata":{}},{"cell_type":"code","source":"day = 96\n#last 3 days for testing and the rest for training\ntarget_test = target[-3*day:]\ntarget_train = target[:-3*day]\n#moving target back a day\nbaseline_preds = target.shift(day)[-3*day:]\nbaseline_score = np.round(mean_squared_error(target_test, baseline_preds, squared=False), 4)\nprint('baseline_score: ', baseline_score)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.273198Z","iopub.status.idle":"2023-06-21T23:13:38.273556Z","shell.execute_reply.started":"2023-06-21T23:13:38.273358Z","shell.execute_reply":"2023-06-21T23:13:38.273373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\n# plt.plot(X[X.index > '2020-06-10'].index, y_pred[X.index > '2020-06-10'], alpha=0.5)\nplt.plot(baseline_preds, label='baseline')\nplt.plot(target_test);\nplt.title(f'Baseline RMSE Score {baseline_score}')\nplt.legend(['Baseline preds', 'Actual']);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.274422Z","iopub.status.idle":"2023-06-21T23:13:38.274790Z","shell.execute_reply.started":"2023-06-21T23:13:38.274600Z","shell.execute_reply":"2023-06-21T23:13:38.274617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is actually not too bad. It does capture some stretches pretty accurately. Likely the day before data will play a prominent role in our model.","metadata":{}},{"cell_type":"markdown","source":"## How to handle inverters? / Weather impact\nSo far we have had all of the inverters in one column of our dataframe with all of the their individual generation data in additional rows. For modeling though we should aggregate our dataframe to one datatime and make that our index. This leaves the question of what to do with the inverters. We could: aggregate accross them, greatly simplifying our dataframe, but possibly losing helpful information; or pivot them as new features to our dataframe making it much more complex; model each inverter seperately and make our model the sum of their predicitons. Also, as per our project scope, let's use this chance to see how the weather data impacts the models performance. We'll setup for each scenerio below","metadata":{}},{"cell_type":"code","source":"generation = ['AC_POWER', 'DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD']\nweather = ['AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']\nall_features = ['AC_POWER', 'DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']\n#Aggregating data across inverters. We will sum the generation data and take the mean of the weather data. Also, adding back basic datetime features\ndf_agg = pd.concat([df.groupby('DATE_TIME')[generation].sum(numeric_only=True), df.groupby('DATE_TIME').mean(numeric_only=True)[weather]], axis=1)\n\ndf_agg['HOUR'] = df_agg.index.hour\ndf_agg['DAY'] = df_agg.index.dayofyear\ndf_agg['DAY_WEEK'] = df_agg.index.dayofweek\ndf_agg['MINUTES_15'] = df_agg.index.time\n\n#This maps the 15 minute intervals over the course of the day to ints 1-96. \ndf_agg['MINUTES_15'] = df_agg.MINUTES_15.map({df_agg.MINUTES_15.unique()[i-1]:i for i in range(1, 97)})\n#Change day of year to day of data\ndf_agg['DAY'] = df_agg.DAY.map({df_agg.DAY.unique()[i-1]:i for i in range(1, 35)})\n#Pivoting inverter generation data as features and adding back datatime features\ninverter_dfs = []\nfor i in inverters:\n    df_inverter = df[df.INVERTER == i][['INVERTER', 'DATE_TIME', 'AC_POWER', 'DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD']]\n    df_inverter.pop('INVERTER')\n    date_time = df_inverter.pop('DATE_TIME')\n    df_inverter.index = date_time\n    df_inverter.columns += f'_{i}'\n    inverter_dfs.append(df_inverter)\n    \ndf_sep = pd.concat(inverter_dfs, axis=1, ignore_index=False)\ndf_sep = pd.concat([df_sep, df_agg], axis=1)\n\nall_features_seperated = list(df_sep.columns)\n\n#Without the weather data\ndf_agg_no_weather = df_agg.drop(weather, axis=1)\ndf_sep_no_weather = df_sep.drop(weather, axis=1)\n\n#For deep learning model\ndf_agg_dl = df_agg.copy()\ndf_sep_dl = df_sep.copy()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.276315Z","iopub.status.idle":"2023-06-21T23:13:38.276725Z","shell.execute_reply.started":"2023-06-21T23:13:38.276525Z","shell.execute_reply":"2023-06-21T23:13:38.276543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection\nFinally, we have made it to selecting our model. Per the project scope were are going to use an XGBoost algorithm. XGBoost is chosen since it is known to have generally good results and is quick to train. ","metadata":{}},{"cell_type":"markdown","source":"### CV\nWe'll use GridSearchCV to both help tune our model and handle the cross-validation for us. Since we are of course using time series data the CV needs to be sensitive to target leakage by only testing on future indices. Luckily, sklearn has a function to help us. We'll leave off the last 4 days for future use (to have 3 days of day ahead testing). Setting the test size to 96 will have it test one day ahead for a full day at a time. Also, let's have no limit on the train size to make it an expanding training window. This might help the model capture some longer term trends.","metadata":{}},{"cell_type":"code","source":"day = 96\ntscv = TimeSeriesSplit(test_size=day, n_splits=29)\ncv_indices = []\n#since all our inverter splits share the same index we'll just choose df_agg\nfor train_indices, test_indices in tscv.split(df_agg.iloc[:-day*4]):\n    cv_indices.append((train_indices, test_indices))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.278243Z","iopub.status.idle":"2023-06-21T23:13:38.278634Z","shell.execute_reply.started":"2023-06-21T23:13:38.278449Z","shell.execute_reply":"2023-06-21T23:13:38.278467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GridSearch\nNow with our CV indices in place are ready to train and test our first models. Let's start with a very basic grid search for our XGBoost regressor and the data aggregated across the inverters. We'll also make a dictionary to store our results.","metadata":{}},{"cell_type":"code","source":"results = {}\n#making train and test sets for our different inverter splits and with and without the weather data\ndf_agg_no_weather_train = df_agg_no_weather.iloc[:-day*4]\ndf_agg_no_weather_test = df_agg_no_weather.iloc[-day*4:-day]\n\ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]\n\ndf_sep_no_weather_train = df_sep_no_weather.iloc[:-day*4]\ndf_sep_no_weather_test = df_sep_no_weather.iloc[-day*4:-day]\n\ndf_sep_train = df_sep.iloc[:-day*4]\ndf_sep_test = df_sep.iloc[-day*4:-day]\n\nxgb = XGBRegressor(random_state=1)\n#param_grid = {'booster': ['dart', 'gbtree', 'gblinear'], 'n_estimators': [50, 100, 150], 'eta': [0.05, 0.1, 0.3]}\nparam_grid = {}\ngs = GridSearchCV(xgb, param_grid=param_grid, cv=cv_indices, verbose=10, scoring='neg_root_mean_squared_error')\ngs.fit(df_agg_no_weather_train, target_train)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.280144Z","iopub.status.idle":"2023-06-21T23:13:38.280568Z","shell.execute_reply.started":"2023-06-21T23:13:38.280351Z","shell.execute_reply":"2023-06-21T23:13:38.280369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(\"CV RMSE: \", -np.round(gs.best_score_, 4), \"CV RMSE STD: \",\nnp.round(gs.cv_results_['std_test_score'][0], 4))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.282131Z","iopub.status.idle":"2023-06-21T23:13:38.282544Z","shell.execute_reply.started":"2023-06-21T23:13:38.282318Z","shell.execute_reply":"2023-06-21T23:13:38.282335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_xgb = gs.best_estimator_\npd.DataFrame(best_xgb.feature_importances_.T, index=list(best_xgb.feature_names_in_), columns=['Feature_Importance']).sort_values('Feature_Importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.283655Z","iopub.status.idle":"2023-06-21T23:13:38.284071Z","shell.execute_reply.started":"2023-06-21T23:13:38.283883Z","shell.execute_reply":"2023-06-21T23:13:38.283901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_no_weather_preds = best_xgb.predict(df_agg_no_weather_test)\nagg_no_weather_score = np.round(mean_squared_error(target_test, agg_no_weather_preds, squared=False), 4)\nprint('RMSE 3-day test period ', agg_no_weather_score)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.285416Z","iopub.status.idle":"2023-06-21T23:13:38.285796Z","shell.execute_reply.started":"2023-06-21T23:13:38.285602Z","shell.execute_reply":"2023-06-21T23:13:38.285619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\n# plt.plot(X[X.index > '2020-06-10'].index, y_pred[X.index > '2020-06-10'], alpha=0.5)\nplt.plot(df_agg_test.index, np.array(agg_no_weather_preds), label='agg')\nplt.plot(target_test.index, np.array(target_test));\nplt.title(f'Agg no weather RMSE Score {agg_no_weather_score}')\nplt.legend(['Agg no weather preds', 'Actual']);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.287850Z","iopub.status.idle":"2023-06-21T23:13:38.288275Z","shell.execute_reply.started":"2023-06-21T23:13:38.288052Z","shell.execute_reply":"2023-06-21T23:13:38.288070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's streamline this a bit and make this a function since we will be train our model several times.","metadata":{}},{"cell_type":"code","source":"def model_train_plot(df_train, df_test, target_train, target_test, name, results_dict ={}, cv_indices = cv_indices, param_grid = {}, estimator=XGBRegressor(random_state=1)):\n    results_dict = results_dict\n    gs = GridSearchCV(xgb, param_grid=param_grid, cv=cv_indices, scoring='neg_root_mean_squared_error', verbose=0)\n    gs.fit(df_train, target_train);\n    \n    params_tried = gs.param_grid\n    best_params = gs.best_params_\n    rmse_score_cv = -np.round(gs.best_score_, 4)\n    std_score_cv = np.round(gs.cv_results_['std_test_score'][0], 4)\n\n    best_estimator = gs.best_estimator_\n    feature_importances_df = pd.DataFrame(best_estimator.feature_importances_.T, index=list(best_estimator.feature_names_in_), columns=['Feature_Importance']).sort_values('Feature_Importance', ascending=False)\n\n    model_preds = best_estimator.predict(df_test)\n#     model_preds['AC_POWER'] = model_preds['AC_POWER'].mask(df.HOUR.isin([0]), 0)\n    rmse_score_test = np.round(mean_squared_error(target_test, model_preds, squared=False), 4)\n    #day_ahead_rmse_score_test = np.round(mean_squared_error(target_test.iloc[:day], model_preds[:day], squared=False), 4)\n    \n\n    fig, ax = plt.subplots(figsize=(15,5))\n    plt.plot(df_test.index, np.array(model_preds), label='agg')\n    plt.plot(target_test.index, np.array(target_test));\n    plt.title(f'AC Power {name}_preds vs actual')\n    plt.ylabel('AC Power (kW)')\n    plt.xlabel('Day')\n#     plt.figtext(.13, .85, s=f'Single Day Ahead RMSE Score test set: {day_ahead_rmse_score_test}')\n    plt.figtext(.13, .85, s=f'RMSE Score test set: {rmse_score_test}')\n    plt.figtext(.13, .82, s=f'CV RMSE Score: {rmse_score_cv}')\n    plt.figtext(.13, .79, s=f'CV STD Score: {std_score_cv}')\n    plt.figtext(.13, .76, s=f'Best Params: {best_params}')\n    plt.legend([f'{name} preds', 'Actual'], loc='upper right');\n    results_dict.update({f'{name}':{'best_params':best_params, 'params_tried':params_tried, 'rmse_score_cv': rmse_score_cv, 'std_score_cv':std_score_cv, 'rmse_score_test': rmse_score_test}})\n    return fig, gs, results_dict, feature_importances_df.T, model_preds","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.289877Z","iopub.status.idle":"2023-06-21T23:13:38.290269Z","shell.execute_reply.started":"2023-06-21T23:13:38.290078Z","shell.execute_reply":"2023-06-21T23:13:38.290095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have a training funtion in place let's call it with each of the four scenarios. We will also give it a basic param_grid to hopefully give each scenario a fair chance.","metadata":{}},{"cell_type":"code","source":"# param_grid = {'eta': [0.05, 0.1, 0.3], 'n_estimators': [50, 100, 150]}\n# fig, gs_agg_no_weather, results, feat, preds = model_train_plot(df_agg_no_weather_train, df_agg_no_weather_test, target_train, target_test, 'Aggregated_no_weather_gen', param_grid=param_grid)\n# feat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.291527Z","iopub.status.idle":"2023-06-21T23:13:38.291906Z","shell.execute_reply.started":"2023-06-21T23:13:38.291714Z","shell.execute_reply":"2023-06-21T23:13:38.291731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, gs_agg, results, feat, preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen', param_grid=param_grid)\n# feat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.293162Z","iopub.status.idle":"2023-06-21T23:13:38.293586Z","shell.execute_reply.started":"2023-06-21T23:13:38.293346Z","shell.execute_reply":"2023-06-21T23:13:38.293363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, gs_sep_no_weather, results, feat, pred = model_train_plot(df_sep_no_weather_train, df_sep_no_weather_test, target_train, target_test, 'Seperated_no_weather_gen', param_grid=param_grid)\n# feat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.296932Z","iopub.status.idle":"2023-06-21T23:13:38.297633Z","shell.execute_reply.started":"2023-06-21T23:13:38.297296Z","shell.execute_reply":"2023-06-21T23:13:38.297325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, gs_sep, results, feat, preds = model_train_plot(df_sep_train, df_sep_test, target_train, target_test, 'Seperated_gen', param_grid=param_grid)\n# feat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.299687Z","iopub.status.idle":"2023-06-21T23:13:38.300303Z","shell.execute_reply.started":"2023-06-21T23:13:38.299993Z","shell.execute_reply":"2023-06-21T23:13:38.300020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of them outperformed the baseline on the 3 day test, though not by much. It is close between the model with only the aggregated generation data and the aggregated generation data that also includes the weather. The aggregated model with the weather does seem to do a better job with tracking the more subtle peaks and valleys in our target though.\n\nSeperating the inverters out doesn't seem to help the model. The cause of this is possibly that the extra features overload the model and it has trouble grasping the most important ones. This is evident in the feature importances in models that include all the inverters seperate. It feels a bit arbitrary, which inverters were given the most importance by the model. Also, there doesn't seem to be any improvement on the the day after the model trains with the additional features (single day ahead RMSE test). It will be interesting to see have the inverters seperate is something our deep learning model can make use of later, but for now will just use the aggregated generation data\n\nSo let's pick the aggregated generation data with weather data. This model should be a nice starting point and since it is one of the simpler scenarios it will be quicker to train and give us more bandwidth to engineer additional features.","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering\nNow that we have an idea of how model is performing with just some basic datetime features, let's see how we can improve the performance with adding additional features.","metadata":{}},{"cell_type":"markdown","source":"### SIN/COS features\nFirst we'll make a cosine and sine representation of the day. This technique was taken from this times series <a href=\"https://www.tensorflow.org/tutorials/structured_data/time_series#time\">tutorial</a>. It should help the model pick out the clear daily pattern. ","metadata":{}},{"cell_type":"code","source":"#just using the selected dataset that doesn't include weather or seperate generation data for each inverter\ntimestamp_s = df_agg.index.map(pd.Timestamp.timestamp)\nday_s = 24*60*60\ndf_agg['Day_sin'] = np.sin(timestamp_s  * (2 * np.pi / day_s))\ndf_agg['Day_cos'] = np.cos(timestamp_s * (2 * np.pi / day_s))\n\n#respliting with new features    \ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.302055Z","iopub.status.idle":"2023-06-21T23:13:38.302655Z","shell.execute_reply.started":"2023-06-21T23:13:38.302353Z","shell.execute_reply":"2023-06-21T23:13:38.302378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try our model again with this added feature. We will use the best params from the grid search as well.","metadata":{}},{"cell_type":"code","source":"param_grid = {'eta': [0.05], 'n_estimators': [100]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train,  target_test, name='Aggregated_gen w cos/sin feature', param_grid=param_grid)\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.304480Z","iopub.status.idle":"2023-06-21T23:13:38.305034Z","shell.execute_reply.started":"2023-06-21T23:13:38.304758Z","shell.execute_reply":"2023-06-21T23:13:38.304784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Already a fair bit of improvement and it is by far the most important feature! The cosine tracks much better with our target (see below). Let's generalize our sin/cos and make some new features with different frequencies. Perhaps we can pick up on some additional trends/patterns in our model.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplot()\nsns.lineplot(df_agg['Day_cos'].iloc[:4*day])\nsns.lineplot(df_agg['Day_sin'].iloc[:4*day])\nsns.lineplot((df_agg['AC_POWER'].iloc[:4*day] - df_agg['AC_POWER'].mean())/ df_agg['AC_POWER'].std())\nfig.set_ylabel('')\nfig.set_xlabel('')\nfig.set_yticklabels('')\nfig.set_xticklabels('')\nfig.legend(['Cos'])\nplt.title('Cos/Sin/AC Power');\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.307149Z","iopub.status.idle":"2023-06-21T23:13:38.307726Z","shell.execute_reply.started":"2023-06-21T23:13:38.307448Z","shell.execute_reply":"2023-06-21T23:13:38.307474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows how cosine does a better job of tracking with the AC power. Let's remove the sin feature. We can also make the cosine negative to better align with the shape of the power generation.","metadata":{}},{"cell_type":"code","source":"df_agg = df_agg.drop('Day_sin', axis=1)\ndf_agg['Day_cos'] = -df_agg.Day_cos\n\n#respliting with new features    \ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]\n\n#retraining \nparam_grid = {'eta': [0.05], 'n_estimators': [100]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen w/ cos feature', param_grid=param_grid)\nfeat\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.309292Z","iopub.status.idle":"2023-06-21T23:13:38.313035Z","shell.execute_reply.started":"2023-06-21T23:13:38.312730Z","shell.execute_reply":"2023-06-21T23:13:38.312769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is a bit better. Let's generalize the cos/sin features some and add some more frequencies to see if we can pick up on additional trends.","metadata":{}},{"cell_type":"code","source":"def day_cos_sin(df, freq=1, type='both'):\n    timestamp_s = df.index.map(pd.Timestamp.timestamp)\n    day_s = 24*60*60\n    if type=='both':\n        df[f'Day_sin_{freq}'] = np.sin(timestamp_s  * (2 * freq * np.pi / day_s))\n        df[f'Day_cos_{freq}'] = np.cos(timestamp_s * (2 * freq * np.pi / day_s))\n    elif type=='cos':\n        df[f'Day_cos_{freq}'] = np.cos(timestamp_s * (2 * freq * np.pi / day_s))\n    elif type=='sin':\n        df[f'Day_sin_{freq}'] = np.sin(timestamp_s  * (2 * freq * np.pi / day_s))\n    return df\n\n#We will just add cos features and go up to a 'week'\ndf_agg = day_cos_sin(df_agg, .5, type='cos')\ndf_agg = day_cos_sin(df_agg, 2, type='cos')\ndf_agg = day_cos_sin(df_agg, 3, type='cos')\ndf_agg = day_cos_sin(df_agg, 4, type='cos')\ndf_agg = day_cos_sin(df_agg, 5, type='cos')\ndf_agg = day_cos_sin(df_agg, 6, type='cos')\ndf_agg = day_cos_sin(df_agg, 7, type='cos')\n\n#respliting with new features    \ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.314143Z","iopub.status.idle":"2023-06-21T23:13:38.314575Z","shell.execute_reply.started":"2023-06-21T23:13:38.314346Z","shell.execute_reply":"2023-06-21T23:13:38.314364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'eta': [0.05], 'n_estimators': [100]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen w/ cos features ', param_grid=param_grid)\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.315534Z","iopub.status.idle":"2023-06-21T23:13:38.315918Z","shell.execute_reply.started":"2023-06-21T23:13:38.315729Z","shell.execute_reply":"2023-06-21T23:13:38.315752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cosine features over 1-4 days seem somewhat helpful, but not as much after that. This suggests that our data have useful information up to a period of 4 days. Let's remove the cosine features beyond 4 days and the half-day feature of 0.5.","metadata":{}},{"cell_type":"code","source":"df_agg = df_agg.drop(['Day_cos_0.5', 'Day_cos_5', 'Day_cos_6', 'Day_cos_7'], axis=1)\n\n#respliting with new features    \ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]\n\nparam_grid = {'eta': [0.05], 'n_estimators': [100]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen w cos up to 4 days', param_grid=param_grid)\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.317643Z","iopub.status.idle":"2023-06-21T23:13:38.318143Z","shell.execute_reply.started":"2023-06-21T23:13:38.317950Z","shell.execute_reply":"2023-06-21T23:13:38.317969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lag and rolling features","metadata":{}},{"cell_type":"markdown","source":"Now will add a few more kinds of features. We'll focus these in on the generation data (really no reason to add them to the datetime features). Let's start with a lag feature. Since our cosine features were picking up some potentially useful information up to a 4 day period, we'll make a lag feature going back 4 days. This will introduce some nulls for these features at the beginning of the dataset as we shift the lag features forward, but our model should be able to handle them (see xgboost FAQ: https://xgboost.readthedocs.io/en/stable/faq.html).","metadata":{}},{"cell_type":"code","source":"for f in all_features:\n    new_features = [df_agg]\n    for i in range(day, day*4, day):\n        lag_feature= df_agg[f].shift(i)\n        lag_feature.name = f'lag_{f}{i}'\n        new_features.append(lag_feature)\n    df_agg = pd.concat(new_features, axis=1)\n    \n#respliting with new features    \ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]\n    \n#retraining with new features\nparam_grid = {'eta': [0.05], 'n_estimators': [100]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, name='Aggregated_gen w/ lag features', param_grid=param_grid)\nfeat\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.319345Z","iopub.status.idle":"2023-06-21T23:13:38.319731Z","shell.execute_reply.started":"2023-06-21T23:13:38.319542Z","shell.execute_reply":"2023-06-21T23:13:38.319559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This does seem to help some overall. Now we'll add some rolling features. This should give our model a smoother picture of the previous generation data to work with. Similiar to when we resampled the target above to hourly and it removed much of the noise.","metadata":{}},{"cell_type":"code","source":"for f in all_features:\n    new_features = [df_agg]\n    lag_feature= df_agg[f].rolling(2).mean()\n    lag_feature.name = f'rolling_{f}'\n    new_features.append(lag_feature)\n    df_agg = pd.concat(new_features, axis=1)\n\nfor f in all_features:\n    new_features = [df_agg]\n    lag_feature= df_agg[f].shift(day).rolling(5, center=True).mean()\n    lag_feature.name = f'rolling_{f}_day'\n    new_features.append(lag_feature)\n    df_agg = pd.concat(new_features, axis=1)\n\nfor f in all_features:\n    new_features = [df_agg]\n    lag_feature= df_agg[f].shift(2*day).rolling(5, center=True).mean()\n    lag_feature.name = f'rolling_{f}_day_2'\n    new_features.append(lag_feature)\n    df_agg = pd.concat(new_features, axis=1)\n    \nfor f in all_features:\n    new_features = [df_agg]\n    lag_feature= df_agg[f].shift(3*day).rolling(5, center=True).mean()\n    lag_feature.name = f'rolling_{f}_day_3'\n    new_features.append(lag_feature)\n    df_agg = pd.concat(new_features, axis=1)\n    \n#respliting with new features    \ndf_agg_train = df_agg.iloc[:-day*4]\ndf_agg_test = df_agg.iloc[-day*4:-day]\n\n#retraining with new features\nparam_grid = {'eta': [0.05], 'n_estimators': [100]}\nfig, gs_agg, results, feat, model_preds= model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen w more features', param_grid=param_grid)\nfeat\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.320969Z","iopub.status.idle":"2023-06-21T23:13:38.321331Z","shell.execute_reply.started":"2023-06-21T23:13:38.321151Z","shell.execute_reply":"2023-06-21T23:13:38.321169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perhaps some performance gains, but it is hard to tell for sure. We might have too many features now and/or need to retune the parameters to account for the additional features the model has to process. Let's start with retuning the parameters","metadata":{}},{"cell_type":"markdown","source":"## Tuning parameters","metadata":{}},{"cell_type":"markdown","source":"Let's do a couple rounds of gridsearch including some additional parameters to see if we can improve the performance","metadata":{}},{"cell_type":"code","source":"#1st params tried param_grid = {'booster' : ['dart', 'gbtree'], 'n_estimators': [50, 100, 150], 'eta': [0.01, 0.5, 0.1] }\n#best params {'booster' : ['gbtree'], 'n_estimators': [50], 'eta': [0.1] }\n# Additional parameters\n# 2nd param_grid tried = {'booster' : ['gbtree'], 'max_depth': [3, 5, 10], 'colsample_bytree': [0.5, 0.75, 1], 'subsample': [0.6, 0.8, 1], 'n_estimators' : [50], 'eta' : [0.1]}\n#best params {'booster' : ['gbtree'], 'max_depth':  [3], 'colsample_bytree': [1], 'subsample': [0.8]}\n#3rd param_grid tried param_grid = {'booster' : ['gbtree'], 'max_depth': [1, 2, 3], 'colsample_bytree': [1], 'subsample': [0.8], 'n_estimators': [50], 'eta': [0.1]}\nparam_grid = {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.8], 'n_estimators': [50], 'eta': [0.1]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen tune', param_grid=param_grid)\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.322553Z","iopub.status.idle":"2023-06-21T23:13:38.322923Z","shell.execute_reply.started":"2023-06-21T23:13:38.322743Z","shell.execute_reply":"2023-06-21T23:13:38.322760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a pretty big gain in performance with the max depth set to 1. The model becomes a fair bit simpler and it is less likely to find every peak or valley, but on the average does a decent job. Depending on the exact application this could be underfitting, but since it works well with our chosen metric we'll stick with it.","metadata":{}},{"cell_type":"markdown","source":"### Feature selection with Random Forest","metadata":{}},{"cell_type":"markdown","source":"To simplfy our model even further and see if we can help it focus on the more important features, let's use a random forest model to filter out some of the less helpful features.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestRegressor(random_state=1)\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen random forest', estimator=rfc, param_grid={})\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.324604Z","iopub.status.idle":"2023-06-21T23:13:38.325022Z","shell.execute_reply.started":"2023-06-21T23:13:38.324831Z","shell.execute_reply":"2023-06-21T23:13:38.324848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SelectFromModel(gs_agg.best_estimator_, prefit=True, threshold=0.001)\n\ndf_agg_new = pd.DataFrame(model.transform(df_agg), columns=model.get_feature_names_out(df_agg.columns), index=df_agg.index)\n\ndf_agg_train = df_agg_new.iloc[:-day*4]\ndf_agg_test = df_agg_new.iloc[-day*4:-day]\n\nparam_grid = {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.8], 'n_estimators': [50], 'eta': [0.1]}\nfig, gs_agg, results, feat, model_preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen feature selection', param_grid=param_grid)\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.326094Z","iopub.status.idle":"2023-06-21T23:13:38.326532Z","shell.execute_reply.started":"2023-06-21T23:13:38.326289Z","shell.execute_reply":"2023-06-21T23:13:38.326307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final tuning\nOther than the Day_cos feature the rolling features ended up being the most useful. Since we went with a simpler model that greatly smooths the data over the course of the day, it makes perfect sense that these rolling features played a prominent role. One last fine tuning of params before wrapping up or XGBoost model and moving to the LSTM one.","metadata":{}},{"cell_type":"code","source":"#1st param_grid tried= {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.7, 0.8, 0.9], 'n_estimators': [25, 50, 75], 'eta': [0.75, 0.1, 0.2]}\n#best param_grid on 1st try {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.7], 'n_estimators': [50], 'eta': [0.1]}\n#2nd param_grid tried = {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.65, 0.7, 0.75], 'n_estimators': [45, 50, 55], 'eta': [0.095, 0.1, 0.15]}\n#best param_grid on 2nd try {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.65], 'n_estimators': [45], 'eta': [0.1]}\n#3rd param_grid tried {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.65], 'n_estimators': [42, 43, 44, 45, 46], 'eta': [0.099,0.1,0.11]}\n#best param grid {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.64, 0.65, 0.66], 'n_estimators': [42], 'eta': [0.11]}\n#last try {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.64, 0.65, 0.66], 'n_estimators': [38, 39, 40, 41, 42], 'eta': [0.11, 0.12]}\n#best param below\nparam_grid = {'booster' : ['gbtree'], 'max_depth': [1], 'colsample_bytree': [1], 'subsample': [0.65], 'n_estimators': [42], 'eta': [0.11]}\nfig, gs_agg, results, feat, preds = model_train_plot(df_agg_train, df_agg_test, target_train, target_test, 'Aggregated_gen final tune', param_grid=param_grid)\nfeat","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.328657Z","iopub.status.idle":"2023-06-21T23:13:38.329034Z","shell.execute_reply.started":"2023-06-21T23:13:38.328849Z","shell.execute_reply":"2023-06-21T23:13:38.328867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Onto Deep learning","metadata":{}},{"cell_type":"markdown","source":"Now that we have completed our XGBoost model, let's move to our deep learning model. For our deep learning model we will use a LSTM layer to make a recurrent model (RNN). This should allow our model to store helpful information about that past to use for its forecast. It will take in the data from the last 4 days and generate a forecast of AC Power for each 15 minute interval for the next day. \n\nMost of the preprocessing has already been finished before, but we additionally need to normalize the input data. Also, we'll go back check to see how it performs using the inverters seperated and aggregated. We'll include the weather data in both. My guess is that our deep learning model might be able to make better use of the additional features and show some improvement with the inverters seperated. First, we have to setup the data to feed into our deep learning model.","metadata":{}},{"cell_type":"code","source":"#removing features (besides basic datetime ones)\ndf_agg = df_agg_dl\ndf_sep = df_sep_dl\nday = 96\ntscv = TimeSeriesSplit(test_size=day, n_splits=29, max_train_size=4*day)\ncv_indices = []\n#since all our inverter splits share the same index we'll just choose df_agg\nfor train_indices, test_indices in tscv.split(df_agg.iloc[:-day*4]):\n    cv_indices.append((train_indices, test_indices))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.330344Z","iopub.status.idle":"2023-06-21T23:13:38.330779Z","shell.execute_reply.started":"2023-06-21T23:13:38.330588Z","shell.execute_reply":"2023-06-21T23:13:38.330607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### df_sep full data including weather\n\ninputs_sep_trains = []\nlabels_sep_trains = []\nfor i in range(3, len(cv_indices)-4):\n    input_sep_train = (df_sep.iloc[cv_indices[i][0]] - df_sep.iloc[cv_indices[i][0]].mean()) / df_sep.iloc[cv_indices[i][0]].std()\n    label_sep_train = (target.iloc[cv_indices[i][1]] - target.iloc[cv_indices[i][1]].mean()) / target.iloc[cv_indices[i][1]].std()\n    label_sep_train = pd.DataFrame(label_sep_train, columns=['AC_POWER'])\n    inputs_sep_trains.append(input_sep_train)\n    labels_sep_trains.append(label_sep_train)\n    \ninputs_sep_train_stack = tf.stack(inputs_sep_trains)\nlabels_sep_train_stack = tf.stack(labels_sep_trains)\n\ninputs_sep_vals = []\nlabels_sep_vals = []\nfor i in range(len(cv_indices)-4 , len(cv_indices)):\n    input_sep_val = (df_sep.iloc[cv_indices[i][0]] - df_sep.iloc[cv_indices[i][0]].mean()) / df_sep.iloc[cv_indices[i][0]].std()\n    label_sep_val = (target.iloc[cv_indices[i][1]] - target.iloc[cv_indices[i][1]].mean()) / target.iloc[cv_indices[i][1]].std()\n    label_sep_val = pd.DataFrame(label_sep_val, columns=['AC_POWER'])\n    inputs_sep_vals.append(input_sep_val)\n    labels_sep_vals.append(label_sep_val)\n    \ninputs_sep_val_stack = tf.stack(inputs_sep_vals)\nlabels_sep_val_stack = tf.stack(labels_sep_vals)\n    \nval = (inputs_sep_val_stack, labels_sep_val_stack)\n ","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.332024Z","iopub.status.idle":"2023-06-21T23:13:38.332937Z","shell.execute_reply.started":"2023-06-21T23:13:38.332728Z","shell.execute_reply":"2023-06-21T23:13:38.332748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"### Multi output sep\nMAX_EPOCHS = 2000\n\nmodel = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(10, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(96*1,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([96, 1])\n])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=20,\n                                                    mode='min',\n                                                    restore_best_weights=True)\n\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nhistory = model.fit(x=inputs_sep_train_stack, y=labels_sep_train_stack, epochs=MAX_EPOCHS,\n                    validation_data=val,\n                    callbacks=[early_stopping],\n                    verbose=0);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.334004Z","iopub.status.idle":"2023-06-21T23:13:38.334382Z","shell.execute_reply.started":"2023-06-21T23:13:38.334196Z","shell.execute_reply":"2023-06-21T23:13:38.334213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_1_labels = target.iloc[-day*3:-day*2]\nday_2_labels = target.iloc[-day*2:-day*1]\nday_3_labels = target.iloc[-day:]\n\ntest_labels = [day_1_labels, day_2_labels, day_3_labels]\ntest_labels_stack = tf.stack(test_labels)\n\nday_1_inputs = (df_sep.iloc[-day*7:-day*3] - df_sep.iloc[-day*7:-day*3].mean()) / df_sep.iloc[-day*7:-day*3].std()\nday_2_inputs = (df_sep.iloc[-day*6:-day*2] - df_sep.iloc[-day*6:-day*2].mean()) / df_sep.iloc[-day*6:-day*2].std()\nday_3_inputs = (df_sep.iloc[-day*5:-day] - df_sep.iloc[-day*5:-day].mean()) / df_sep.iloc[-day*5:-day].std()\n\ntest_inputs = [day_1_inputs, day_2_inputs, day_3_inputs]\ntest_inputs_stack = tf.stack(test_inputs)\n\ntarget_norm_preds = model.predict(test_inputs_stack)\n\nlabels_preds_day_1 = target_norm_preds[0] * df_sep.iloc[-day*7:-day*3].AC_POWER.std() + df_sep.iloc[-day*7:-day*3].AC_POWER.mean()\nlabels_preds_day_2 = target_norm_preds[0] * df_sep.iloc[-day*6:-day*2].AC_POWER.std() + df_sep.iloc[-day*6:-day*2].AC_POWER.mean()\nlabels_preds_day_3 = target_norm_preds[0] * df_sep.iloc[-day*5:-day].AC_POWER.std() + df_sep.iloc[-day*5:-day].AC_POWER.mean()\ntest_labels_stack[0]\n\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(labels_preds_day_1)\nplt.plot(test_labels_stack[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.335683Z","iopub.status.idle":"2023-06-21T23:13:38.336055Z","shell.execute_reply.started":"2023-06-21T23:13:38.335873Z","shell.execute_reply":"2023-06-21T23:13:38.335891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_1_error = mean_squared_error(test_labels_stack[0], labels_preds_day_1, squared=False)\nday_2_error = mean_squared_error(test_labels_stack[1], labels_preds_day_2, squared=False)\nday_3_error = mean_squared_error(test_labels_stack[2], labels_preds_day_3, squared=False)\nlstm_rsme_score_test_set = np.round((day_1_error + day_2_error + day_3_error) / 3, 4)\nprint(\"LSTM RSME test set:\" , lstm_rsme_score_test_set)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.337631Z","iopub.status.idle":"2023-06-21T23:13:38.338066Z","shell.execute_reply.started":"2023-06-21T23:13:38.337880Z","shell.execute_reply":"2023-06-21T23:13:38.337899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### df_agg full data including weather\ninputs_agg_trains = []\nlabels_agg_trains = []\nfor i in range(3, len(cv_indices)-4):\n    input_agg_train = (df_agg.iloc[cv_indices[i][0]] - df_agg.iloc[cv_indices[i][0]].mean()) / df_agg.iloc[cv_indices[i][0]].std()\n    label_agg_train = (target.iloc[cv_indices[i][1]] - target.iloc[cv_indices[i][1]].mean()) / target.iloc[cv_indices[i][1]].std()\n    label_agg_train = pd.DataFrame(label_sep_train, columns=['AC_POWER'])\n    inputs_agg_trains.append(input_agg_train)\n    labels_agg_trains.append(label_agg_train)\n    \ninputs_agg_train_stack = tf.stack(inputs_agg_trains)\nlabels_agg_train_stack = tf.stack(labels_agg_trains)\n\ninputs_agg_vals = []\nlabels_agg_vals = []\nfor i in range(len(cv_indices)-4 , len(cv_indices)):\n    input_agg_val = (df_agg.iloc[cv_indices[i][0]] - df_agg.iloc[cv_indices[i][0]].mean()) / df_agg.iloc[cv_indices[i][0]].std()\n    label_agg_val = (target.iloc[cv_indices[i][1]] - target.iloc[cv_indices[i][1]].mean()) / target.iloc[cv_indices[i][1]].std()\n    label_agg_val = pd.DataFrame(label_sep_val, columns=['AC_POWER'])\n    inputs_agg_vals.append(input_agg_val)\n    labels_agg_vals.append(label_agg_val)\n    \ninputs_agg_val_stack = tf.stack(inputs_agg_vals)\nlabels_agg_val_stack = tf.stack(labels_agg_vals)\n    \nval = (inputs_agg_val_stack, labels_agg_val_stack)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.339331Z","iopub.status.idle":"2023-06-21T23:13:38.339728Z","shell.execute_reply.started":"2023-06-21T23:13:38.339533Z","shell.execute_reply":"2023-06-21T23:13:38.339550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Multi output sep\nMAX_EPOCHS = 2000\n\nmodel = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(10, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(96*1,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([96, 1])\n])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=20,\n                                                    mode='min',\n                                                    restore_best_weights=True)\n\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nhistory = model.fit(x=inputs_agg_train_stack, y=labels_agg_train_stack, epochs=MAX_EPOCHS,\n                    validation_data=val,\n                    callbacks=[early_stopping],\n                    verbose=0);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.340837Z","iopub.status.idle":"2023-06-21T23:13:38.341210Z","shell.execute_reply.started":"2023-06-21T23:13:38.341018Z","shell.execute_reply":"2023-06-21T23:13:38.341035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_1_labels = target.iloc[-day*3:-day*2]\nday_2_labels = target.iloc[-day*2:-day*1]\nday_3_labels = target.iloc[-day:]\n\ntest_labels = [day_1_labels, day_2_labels, day_3_labels]\ntest_labels_stack = tf.stack(test_labels)\n\nday_1_inputs = (df_agg.iloc[-day*7:-day*3] - df_agg.iloc[-day*7:-day*3].mean()) / df_agg.iloc[-day*7:-day*3].std()\nday_2_inputs = (df_agg.iloc[-day*6:-day*2] - df_agg.iloc[-day*6:-day*2].mean()) / df_agg.iloc[-day*6:-day*2].std()\nday_3_inputs = (df_agg.iloc[-day*5:-day] - df_agg.iloc[-day*5:-day].mean()) / df_agg.iloc[-day*5:-day].std()\n\ntest_inputs = [day_1_inputs, day_2_inputs, day_3_inputs]\ntest_inputs_stack = tf.stack(test_inputs)\n\ntarget_norm_preds = model.predict(test_inputs_stack)\n\nlabels_preds_day_1 = target_norm_preds[0] * df_agg.iloc[-day*7:-day*3].AC_POWER.std() + df_agg.iloc[-day*7:-day*3].AC_POWER.mean()\nlabels_preds_day_2 = target_norm_preds[0] * df_agg.iloc[-day*6:-day*2].AC_POWER.std() + df_agg.iloc[-day*6:-day*2].AC_POWER.mean()\nlabels_preds_day_3 = target_norm_preds[0] * df_agg.iloc[-day*5:-day].AC_POWER.std() + df_agg.iloc[-day*5:-day].AC_POWER.mean()\ntest_labels_stack[0]\n\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(labels_preds_day_1)\nplt.plot(test_labels_stack[0])\nplt.legend(['predicted', 'actual'])","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.346709Z","iopub.status.idle":"2023-06-21T23:13:38.347197Z","shell.execute_reply.started":"2023-06-21T23:13:38.346990Z","shell.execute_reply":"2023-06-21T23:13:38.347012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_1_error = mean_squared_error(test_labels_stack[0], labels_preds_day_1, squared=False)\nday_2_error = mean_squared_error(test_labels_stack[1], labels_preds_day_2, squared=False)\nday_3_error = mean_squared_error(test_labels_stack[2], labels_preds_day_3, squared=False)\nlstm_rsme_score_test_set = np.round((day_1_error + day_2_error + day_3_error) / 3, 4)\nprint(\"LSTM RSME test set aggregated:\" , lstm_rsme_score_test_set)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.349076Z","iopub.status.idle":"2023-06-21T23:13:38.349532Z","shell.execute_reply.started":"2023-06-21T23:13:38.349318Z","shell.execute_reply":"2023-06-21T23:13:38.349338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model with the inverters seperated is the clear winner in this. That shows as predicted that our deep learning model is able to do more with complex data. With this in mind, let's try to add all of the features we created during our previous feature engineering. Since this will include the seperate inverter features it could well overload it, but it will be interesting to see.","metadata":{}},{"cell_type":"code","source":"df_sep = df_sep_dl\n\ndf_sep = day_cos_sin(df_sep, 1, type='cos')\ndf_sep = day_cos_sin(df_sep, 2, type='cos')\ndf_sep = day_cos_sin(df_sep, 3, type='cos')\ndf_sep = day_cos_sin(df_sep, 4, type='cos')\n\nfor f in all_features_seperated:\n    new_features = [df_sep]\n    for i in range(day, day*4, day):\n        lag_feature= df_sep[f].shift(i)\n        lag_feature.name = f'lag_{f}{i}'\n        new_features.append(lag_feature)\n    df_sep = pd.concat(new_features, axis=1)\n    \nfor f in all_features_seperated:\n    new_features = [df_sep]\n    lag_feature= df_sep[f].rolling(2).mean()\n    lag_feature.name = f'rolling_{f}'\n    new_features.append(lag_feature)\n    df_sep = pd.concat(new_features, axis=1)\n\nfor f in all_features_seperated:\n    new_features = [df_sep]\n    lag_feature= df_sep[f].shift(day).rolling(5, center=True).mean()\n    lag_feature.name = f'rolling_{f}_day'\n    new_features.append(lag_feature)\n    df_sep = pd.concat(new_features, axis=1)\n\nfor f in all_features_seperated:\n    new_features = [df_sep]\n    lag_feature= df_sep[f].shift(2*day).rolling(5, center=True).mean()\n    lag_feature.name = f'rolling_{f}_day_2'\n    new_features.append(lag_feature)\n    df_sep = pd.concat(new_features, axis=1)\n    \nfor f in all_features_seperated:\n    new_features = [df_sep]\n    lag_feature= df_sep[f].shift(3*day).rolling(5, center=True).mean()\n    lag_feature.name = f'rolling_{f}_day_3'\n    new_features.append(lag_feature)\n    df_sep = pd.concat(new_features, axis=1)\n    \nimputer = KNNImputer(n_neighbors=10)\ndf_sep = pd.DataFrame(imputer.fit_transform(df_sep), columns=df_sep.columns, index=df_sep.index)\ndf_sep.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.350899Z","iopub.status.idle":"2023-06-21T23:13:38.351274Z","shell.execute_reply.started":"2023-06-21T23:13:38.351088Z","shell.execute_reply":"2023-06-21T23:13:38.351106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### df_sep full data including all created features\ninputs_sep_full_trains = []\nlabels_sep_full_trains = []\nfor i in range(4, len(cv_indices)-4):\n    input_sep_full_train = (df_sep.iloc[cv_indices[i][0]] - df_sep.iloc[cv_indices[i][0]].mean()) / df_sep.iloc[cv_indices[i][0]].std()\n    label_sep_full_train = (target.iloc[cv_indices[i][1]] - target.iloc[cv_indices[i][1]].mean()) / target.iloc[cv_indices[i][1]].std()\n    label_sep_full_train = pd.DataFrame(label_sep_full_train, columns=['AC_POWER'])\n    inputs_sep_full_trains.append(input_sep_full_train)\n    labels_sep_full_trains.append(label_sep_full_train)\n    \ninputs_sep_full_train_stack = tf.stack(inputs_sep_full_trains)\nlabels_sep_full_train_stack = tf.stack(labels_sep_full_trains)\n\ninputs_sep_full_vals = []\nlabels_sep_full_vals = []\nfor i in range(len(cv_indices)-4 , len(cv_indices)):\n    input_sep_full_val = (df_sep.iloc[cv_indices[i][0]] - df_sep.iloc[cv_indices[i][0]].mean()) / df_sep.iloc[cv_indices[i][0]].std()\n    label_sep_full_val = (target.iloc[cv_indices[i][1]] - target.iloc[cv_indices[i][1]].mean()) / target.iloc[cv_indices[i][1]].std()\n    label_sep_full_val = pd.DataFrame(label_sep_full_val, columns=['AC_POWER'])\n    inputs_sep_full_vals.append(input_sep_full_val)\n    labels_sep_full_vals.append(label_sep_full_val)\n    \ninputs_sep_full_val_stack = tf.stack(inputs_sep_full_vals)\nlabels_sep_full_val_stack = tf.stack(labels_sep_full_vals)\n    \nval_full = (inputs_sep_full_val_stack, labels_sep_full_val_stack)\n ","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.352848Z","iopub.status.idle":"2023-06-21T23:13:38.353263Z","shell.execute_reply.started":"2023-06-21T23:13:38.353060Z","shell.execute_reply":"2023-06-21T23:13:38.353079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Multi output sep\nMAX_EPOCHS = 2000\n\nmodel = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(20, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(96*1,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([96, 1])\n])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=20,\n                                                    mode='min',\n                                                    restore_best_weights=True)\n\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nhistory = model.fit(x=inputs_sep_full_train_stack, y=labels_sep_full_train_stack, epochs=MAX_EPOCHS,\n                    validation_data=val_full,\n                    callbacks=[early_stopping],\n                    verbose=2);","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.354636Z","iopub.status.idle":"2023-06-21T23:13:38.355028Z","shell.execute_reply.started":"2023-06-21T23:13:38.354839Z","shell.execute_reply":"2023-06-21T23:13:38.354857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_1_labels = target.iloc[-day*3:-day*2]\nday_2_labels = target.iloc[-day*2:-day*1]\nday_3_labels = target.iloc[-day:]\n\ntest_labels = [day_1_labels, day_2_labels, day_3_labels]\ntest_labels_stack = tf.stack(test_labels)\n\nday_1_inputs = (df_sep.iloc[-day*7:-day*3] - df_sep.iloc[-day*7:-day*3].mean()) / df_sep.iloc[-day*7:-day*3].std()\nday_2_inputs = (df_sep.iloc[-day*6:-day*2] - df_sep.iloc[-day*6:-day*2].mean()) / df_sep.iloc[-day*6:-day*2].std()\nday_3_inputs = (df_sep.iloc[-day*5:-day] - df_sep.iloc[-day*5:-day].mean()) / df_sep.iloc[-day*5:-day].std()\n\ntest_inputs = [day_1_inputs, day_2_inputs, day_3_inputs]\ntest_inputs_stack = tf.stack(test_inputs)\n\ntarget_norm_preds = model.predict(test_inputs_stack)\n\nlabels_preds_day_1 = target_norm_preds[0] * df_sep.iloc[-day*7:-day*3].AC_POWER.std() + df_sep.iloc[-day*7:-day*3].AC_POWER.mean()\nlabels_preds_day_2 = target_norm_preds[0] * df_sep.iloc[-day*6:-day*2].AC_POWER.std() + df_sep.iloc[-day*6:-day*2].AC_POWER.mean()\nlabels_preds_day_3 = target_norm_preds[0] * df_sep.iloc[-day*5:-day].AC_POWER.std() + df_sep.iloc[-day*5:-day].AC_POWER.mean()\ntest_labels_stack[0]\n\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(labels_preds_day_1)\nplt.plot(test_labels_stack[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.356957Z","iopub.status.idle":"2023-06-21T23:13:38.357661Z","shell.execute_reply.started":"2023-06-21T23:13:38.357324Z","shell.execute_reply":"2023-06-21T23:13:38.357354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_1_error = mean_squared_error(test_labels_stack[0], labels_preds_day_1, squared=False)\nday_2_error = mean_squared_error(test_labels_stack[1], labels_preds_day_2, squared=False)\nday_3_error = mean_squared_error(test_labels_stack[2], labels_preds_day_3, squared=False)\nlstm_rsme_score_test_set = np.round((day_1_error + day_2_error + day_3_error) / 3, 4)\nprint(\"LSTM RSME test set:\" , lstm_rsme_score_test_set)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T23:13:38.359969Z","iopub.status.idle":"2023-06-21T23:13:38.360580Z","shell.execute_reply.started":"2023-06-21T23:13:38.360271Z","shell.execute_reply":"2023-06-21T23:13:38.360298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wait, wasn't there another plant?\nYes, let's not forget there is a whole other solar plant and associated weather data to model. We'll take our most successful model so far and make a full pipeline out of to process the 2nd solar plant data and make a day ahead forecast for it","metadata":{}},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make a full pipeline we can ","metadata":{}},{"cell_type":"markdown","source":"## An easier way?","metadata":{}},{"cell_type":"markdown","source":"## Conclusions/ Project extensions","metadata":{}},{"cell_type":"markdown","source":"# Appendix","metadata":{}}]}